{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marios\\anaconda3\\envs\\ML_projects\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import albumentations as albu\n",
    "\n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "\t\t\n",
    "# load data\n",
    "trainX = load_zipped_pickle(\"data/train.pkl\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "trainX_extr = []\n",
    "for i in range(len(trainX)):\n",
    "    video = trainX[i][\"video\"]\n",
    "    frames = trainX[i][\"frames\"]\n",
    "    frame1 = video[:,:,frames[0]]\n",
    "    frame2 = video[:,:,frames[1]]\n",
    "    frame3 = video[:,:,frames[2]]\n",
    "    label1 = trainX[i][\"label\"][:,:,frames[0]]\n",
    "    label2 = trainX[i][\"label\"][:,:,frames[1]]\n",
    "    label3 = trainX[i][\"label\"][:,:,frames[2]]\n",
    "    trainX_extr.append((frame1,label1))\n",
    "    trainX_extr.append((frame2,label2))\n",
    "    trainX_extr.append((frame3,label3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2(BaseDataset):\n",
    "    \"\"\"mitral valve Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['valve']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            data,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.images_fps = [pair[0] for pair in self.data]\n",
    "        self.masks_fps = [pair[1] for pair in self.data]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [1.0]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "#        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        image = self.images_fps[i][:,:,np.newaxis]\n",
    "        mask = self.masks_fps[i]\n",
    "\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        print(np.array(image).shape)\n",
    "        print(np.array(mask).shape)\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            print(\"doing augmentations\")\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            print(\"doing preprocessing\")\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        print(np.array(image).shape)\n",
    "        print(np.array(mask).shape) \n",
    "\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\t\t\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit = [-0.06,0.06],rotate_limit=0.1, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.Resize(320, 320, interpolation=2),\n",
    "        #albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        #albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "\n",
    "        # albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        # albu.IAAPerspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                # albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                # albu.IAASharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                # albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "  \n",
    "def empty_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(320, 320, interpolation=2),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)  \n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before critical point\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'segmentation_models_pytorch' has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 68\u001b[0m\n\u001b[0;32m     60\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([ \n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mdict\u001b[39m(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[0;32m     62\u001b[0m ])\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# create epoch runners \u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# it is a simple loop of iterating over dataloader`s samples\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m train_epoch \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mTrainEpoch(\n\u001b[0;32m     69\u001b[0m     model, \n\u001b[0;32m     70\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss, \n\u001b[0;32m     71\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics, \n\u001b[0;32m     72\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     73\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[0;32m     74\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     77\u001b[0m valid_epoch \u001b[38;5;241m=\u001b[39m smp\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mValidEpoch(\n\u001b[0;32m     78\u001b[0m     model, \n\u001b[0;32m     79\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mway after critical point\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'segmentation_models_pytorch' has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "print(\"before critical point\")\n",
    "ENCODER = \"se_resnext50_32x4d\"#'efficientnet-b3'#\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['valve']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "# model = smp.FPN(\n",
    "    # encoder_name=ENCODER, \n",
    "    # encoder_weights=ENCODER_WEIGHTS, \n",
    "    # classes=len(CLASSES), \n",
    "    # activation=ACTIVATION,\n",
    "    # in_channels = 3\n",
    "# )\n",
    "model = smp.PSPNet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    in_channels = 3\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "\n",
    "train_dataset = Dataset2(\n",
    "     trainX_extr, \n",
    "    # classes=['valve'],\n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "\n",
    "valid_dataset = Dataset2(\n",
    "     trainX_extr[185:], \n",
    "    # classes=['valve'],\n",
    "    augmentation=empty_augmentation(),#get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)#12\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)#4\n",
    "\n",
    "\n",
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "loss = smp.losses.DiceLoss(mode=\"binary\")\n",
    "metrics = [\n",
    "    smp.metrics.iou_score#(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"way after critical point\")\n",
    "#model.load_state_dict(torch.load(\"100epochsfpn.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'albumentations' has no attribute 'IAAAdditiveGaussianNoise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43malbu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIAAAdditiveGaussianNoise\u001b[49m(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'albumentations' has no attribute 'IAAAdditiveGaussianNoise'"
     ]
    }
   ],
   "source": [
    "albu.IAAAdditiveGaussianNoise(p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "best=0\n",
    "nochange=0\n",
    "for i in range(0, 250):\n",
    "\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    nochange+=1\n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        best = i\n",
    "        nochange=0\n",
    "    print(f\"BEST IS FROM EPOCH {best}\")\n",
    "    print(f\"MAX SCORE IS {max_score}\")\n",
    "    print(f\"CURRENT VAL SCORE IS {valid_logs['iou_score']}\")\n",
    "\n",
    "    if nochange>15:\n",
    "        if optimizer.param_groups[0]['lr']>=5e-6:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "            nochange=0\n",
    "    print(f\"LEARNING RATE IS {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QRT_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
